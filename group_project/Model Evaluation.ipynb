{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the results of our trained models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "With so many continuous features in our dataset, using a Linear Regression seemed to be a natural choice.\n",
    "\n",
    "## Additional data cleaning\n",
    "Before training the model, some additional cleaning of our data was required. Some columns which were not exact duplicates, but which referred to very similar real-world phenomena, remained in our dataset, and needed to be removed. This was a straightforward task, as these columns showed up with a >90% correlation rate between them, so it was easy to simply drop, from each pair like this, the feature with the lower correlation to the target feature.\n",
    "\n",
    "However, this does highlight a pitfall in our data gathering methods. Because our data came from so many different sources, there was always a risk of overlap, and with each source naming their features according to different naming conventions, it is important to understand what each feature represents to avoid such duplication.\n",
    "\n",
    "## Feature Selection\n",
    "With these features now dropped, we were able to select promising features to train our Linear Regression model on. These were: \n",
    "* Social Support\n",
    "* Healthy Life Expectancy at Birth\n",
    "* Positive Affect\n",
    "* Democratic Quality\n",
    "* Delivery Quality\n",
    "* Life Expectancy Age 60\n",
    "* Infant Mortality\n",
    "\n",
    "From these promising features, we can establish at a glance that the strongest correlators to Happiness Score involve the quality and availability of healthcare, with government type and aims also playing a part. One could immediately draw the conclusion that a population which feels cared for, whether that is through their healthcare system or through social welfare schemes, tends to rank higher on the happiness scale.\n",
    "\n",
    "### Training the model\n",
    "After performing a split on our dataset to generate a training subset (70%) and a test subset (30%), we take the chosen features and use them to train our Linear Regression model, targeting Happiness Score.\n",
    "\n",
    "## Initial Results\n",
    "Our model's summary initially shows an R^2 value of 0.716, so while it is perhaps not an ideal fit, much of the variance in our data is still explained by our model. The p-values are also very promising, with every feature (except Intercept) being statistically significant with 95% confidence.\n",
    "\n",
    "## Normalisation\n",
    "After normalising our data, we can compare the co-efficient weights of the different features, and establish which are the most important for our model.\n",
    "\n",
    "The two stand-out feature we see in our data are *Social Support* with a co-efficient of 0.3910, and *Life Expectancy at Age 60*, with a very prominent co-efficient of 0.4293.\n",
    "\n",
    "This seems to support the \"at a glance\" theory from above, as they suggest that a population which expects life to continue well into old age and one which knows it can rely on social support in times of hardship, is likely to report a happier life overall.\n",
    "\n",
    "## Standardisation\n",
    "After normalising and standardising our data, we trained a new model on the same features. While the p-values for the features here remained well within the 95% confidence level, the R^2 value of the new model dropped slightly, from 0.716 to 0.683.\n",
    "\n",
    "By examining the co-efficients again, we can see that once again, *Social Support* remains a very high predictor for happiness, with a co-efficient of 0.3099, while the *Life Expectancy at Age 60* has been replaced as the most important factor by *Healthy Life Expectancy at Birth*, which is a subtle, but interesting shift in priorities between models, but which still supports the same theory as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Happiness Score contains continuous values between 1 and 10. The results are roughly normally-distributed, so we took a simple approach and transformed this into a two-category feature, with any values above 5 being classed as \"Happy\", and any below being classed \"Unhappy\".\n",
    "\n",
    "Then, using the same descriptive features as before, we trained a Logistic Regression on 70% of the available data.\n",
    "\n",
    "## Results\n",
    "When this model was tested on the Training set, it resulted in an 81.91% accuracy, and on the Test set it performed slightly better with 83.42% accuracy. These results indicate that the model we have trained is a good fit for the data. However, we should note that because we have changed our target feature's type from continuous to a binary categorical, this could be considered an \"easier\" prediction for our model to make than the prediction for the Linear Regression.\n",
    "\n",
    "Cross validation of our model tells a similar story, with the mean accuracy being 82.14%, and the F1 score mean being 0.797. Both of these are very promising results for our model, and are a good sign that our predictions will be accurate.\n",
    "\n",
    "## Normalisation\n",
    "After normalising the input data, the model was retrained and the same metrics as above taken to evaluate its precision.\n",
    "\n",
    "The normalised data reached an accuracy of 85% on its Training set, and 86.3% on its Test set. Performing a cross-validation on the model shows us a mean accuracy of 84.97%, and an F1 score of 0.833. \n",
    "\n",
    "These are small but nonetheless relevant improvements. However, because the dataset is relatively small, and because the model was retrained on a different random subset of the overall dataset, the difference in outcomes could be attributed to the effect of different training data skewing the model.\n",
    "\n",
    "More interesting to look at however, are the co-efficients used by the Logarithmic Regression model. Matching what we saw above, the three most important features used by the Regression model were *Social Support* (3.4907595), *Life Expectancy at Age 60* (2.13456712), and *Healthy Life Expectancy* (2.12386202)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
